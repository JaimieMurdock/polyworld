#!/usr/local/bin/python

CurrentVersion=2

### Configurable parameters
OutputFilename2='MetricHist.plt'

#CALC_COMPLEXITY_OVER_DIRECTORY="./CalcComplexityOverDirectory.sh"
#### Don't modify anything beneath here unless you know what you're doing
import algorithms
import getopt, glob, string, sys, os
import common_functions
from common_functions import err, warn, list_difference
import common_metric
import datalib
import networkx as nx
import numpy

### Now initialize some global variables
##OutputFilename = common_complexity.FILENAME_AVR
OutputFilename = common_metric.FILENAME_AVR
NUMBINS = common_metric.DEFAULT_NUMBINS
RandomFlag = False
LEGACY_MODES = ['force','prefer','off']
LegacyMode = 'off'
####

####################################################################################
###
### main()
###
####################################################################################
def main():
	#check_environment()

	metrics, recent_type, arg_paths = parse_args(sys.argv[1:])

	recent_subpath = os.path.join('brain', recent_type)
	try:
		run_paths = common_functions.find_run_paths(arg_paths,
							    recent_subpath)
	except common_functions.InvalidDirError, e:
		show_usage(str(e))

	recent_dirs = map(lambda x: os.path.join(x, recent_subpath),
			  run_paths)

	for recent_dir in recent_dirs:
		analyze_recent_dir(metrics,
				   recent_dir)

	print "Done!"
	return 0

####################################################################################
###
### FUNCTION check_environment()
###
####################################################################################
def check_environment():
	global CALC_COMPLEXITY

	CALC_COMPLEXITY = common_functions.pw_env('complexity')

####################################################################################
###
### FUNCTION parse_args()
###
####################################################################################
def parse_args(argv):
	global NUMBINS, LegacyMode, RandomFlag

	if len(argv) == 0:
		show_usage()

	metrics = common_metric.DEFAULT_METRICS#'CC' represents clustering coefficient
	                              #and 'SP' represents shortest path length


##	short = 'C:l:b:'
##	long = ['legacy=', 'bins=']
	short = 'M:R'
	long = ['metrics=','random=']


	try:
		opts, args = getopt.getopt(argv, short, long)
	except getopt.GetoptError, e:
		show_usage(str(e))

	for opt, value in opts:
		opt = opt.strip('-')

		if opt == 'M':
			metrics = map(string.upper, value.split(','))
		elif opt == 'R':
                        RandomFlag = True
##		elif opt == 'l' or opt == 'legacy':
##			if value not in LEGACY_MODES:
##				show_usage('Invalid legacy mode (%s)' % value)
##			LegacyMode = value
##		elif opt == 'b' or opt == 'bins':
##			try:
##				NUMBINS = int(value)
##			except:
##				show_usage("Invalid integer argument for --bins (%s)" % value)
		else:
			assert(False)

	if len(args) == 0:
		show_usage('Must specify recent type.')

	recent_type = args.pop(0)
	if not recent_type in common_functions.RECENT_TYPES:
		show_usage('Invalid recent type (%s).' % recent_type)

	if len(args) == 0:
		show_usage('Must specify run/run-parent directory.')

	paths = list(args)

	return metrics, recent_type, paths

####################################################################################
###
### analyze_recent_dir()
###
####################################################################################
def analyze_recent_dir(metrics, recent_dir):
	outputpath = os.path.join(recent_dir, OutputFilename);
	
	print "- recent directory='%s'" %(recent_dir)
	print "- output='%s'" % (outputpath)
	
	#-----------------------------------------------------------------------------------
	#--
	#-- Find epoch/timestep directories
	#--
	#-----------------------------------------------------------------------------------
	timesteps = []
	# list all of the timesteps, make sure they are all integers (and directories), then sort them by number.
	for potential_timestep in os.listdir( recent_dir ):
		if not potential_timestep.isdigit(): continue					# if timestep IS NOT a digit (note, 0 is considered a digit), skip.
		if not os.path.isdir( os.path.join(recent_dir, potential_timestep) ): continue	# if the timestep isn't a directory, skip it.
	
		timesteps.append( int(potential_timestep) )						# add timestep to our list
	
	if len(timesteps) == 0:
		err('No epochs found. Not a valid recent directory.')

	timesteps.sort()									# sort the timesteps, lowest numbers come first.
	
	#-----------------------------------------------------------------------------------
	#--
	#-- Compute complexities for all timesteps
	#--
	#-- (store values to file in timestep dir)
	#--
	#-----------------------------------------------------------------------------------
	DATA={ }
	
	print "Final Timestep: %s" % ( max(timesteps) )
	print "Processing:",
	for t in timesteps:
		timestep_directory = os.path.join(recent_dir, str(t))
		print '%s...\n' % (t),
		sys.stdout.flush()	
	
		DATA[t] = tdata = {}

		metrics_remaining = metrics

##		if LegacyMode != 'off' :
##			complexities_read = read_legacy_complexities(complexities_remaining,
##								     timestep_directory,
##								     tdata)
##			complexities_remaining = list_difference(complexities_remaining,
##								 complexities_read)
##			if len(complexities_remaining) != 0:
##				if LegacyMode == 'force':
##					err('Failed to find data for %s' % ','.join(complexities_remaining))
##			
##			print '  Legacy =', complexities_read

		if len(metrics_remaining) > 0:
			metrics_computed = compute_metrics(metrics_remaining,
								     timestep_directory,
								     tdata)
			metrics_remaining = list_difference(metrics_remaining,
								 metrics_computed)

		assert(len(metrics_remaining) == 0)
	
	#-----------------------------------------------------------------------------------
	#--
	#-- Create 'Avr' File
	#--
	#-----------------------------------------------------------------------------------
	AVR = algorithms.avr_table(DATA,
                                   metrics,
				   timesteps)
	
	datalib.write(outputpath, AVR)
	
	
##	#-----------------------------------------------------------------------------------
##	#--
##	#-- Create 'Norm' file
##	#--
##	#-----------------------------------------------------------------------------------
##	tables = compute_bins(DATA,
##			      timesteps,
##			      metrics,
##			      AVR,
##			      lambda row: row.get('min'),
##			      lambda row: row.get('max'))
##	
##	outputpath = os.path.join(recent_dir, OutputFilename2.replace( '.', 'Norm.'))
##	
##	datalib.write(outputpath, tables)
##	
##	
##	#-----------------------------------------------------------------------------------
##	#--
##	#-- Create 'Raw' file
##	#--
##	#-----------------------------------------------------------------------------------
##	MAXGLOBAL = dict([(type, float('-inf')) for type in metrics])
##	MINGLOBAL = dict([(type, float('inf')) for type in metrics])
##	
##	for avr_table in AVR.values():
##		for row in avr_table.rows():
##			type = avr_table.name
##	
##			MAXGLOBAL[type] = max(MAXGLOBAL[type], row.get('max'));
##			MINGLOBAL[type] = min(MINGLOBAL[type], row.get('min'));
##	
##	tables = compute_bins(DATA,
##			      timesteps,
##			      metrics,
##			      AVR,
##			      lambda row: MINGLOBAL[row.table.name],
##			      lambda row: MAXGLOBAL[row.table.name])
##	
##	outputpath = os.path.join(recent_dir, OutputFilename2.replace( '.', 'Raw.'))
##	
##	datalib.write(outputpath, tables)
##	
####################################################################################
###
### FUNCTION read_legacy_complexities()
###
####################################################################################
def read_legacy_complexities(complexities,
			     timestep_directory,
			     tdata):
	def __path(type):
		return os.path.join(timestep_directory, 'complexity_' + type + '.txt')

	complexities_read = []
	
	for type in complexities:
		path = __path(type)
		if os.path.isfile(path):
			data = common_complexity.parse_legacy_complexities(path)
			tdata[type] = common_complexity.normalize_complexities(data)
			complexities_read.append(type)

	return complexities_read

####################################################################################
###
### FUNCTION compute_metrics()
###
####################################################################################
def compute_metrics(metrics,
		    timestep_directory,
		    tdata):
	def __path(type):
		return os.path.join(timestep_directory, 'metric_' + type + '.plt')
	
	# --- Read in any metrics computed on a previous invocation of this script
	metrics_read = []
	for type in metrics:
		path = __path(type)

		if os.path.isfile(path):
			try:
				table = datalib.parse(path)[type]
				data = table.getColumn('Metric').data
				tdata[type] = common_metric.normalize_metrics(data)
	
				metrics_read.append(type)
			except datalib.InvalidFileError, e:
				# file must have been incomplete
				print "Failed reading ", path, "(", e, ") ... regenerating"
	
	    
	metrics_remaining = list_difference(metrics,
					    metrics_read)
	
        #metrics_remaining = ['CC','SP']#JUST for TEST
	print "  AlreadyHave =", metrics_read
	print "  MetricsToGet =", metrics_remaining

	# --- Compute metrics not already found on the file system
	if metrics_remaining:
		# --- Execute CalcMetric on all corresponding brainAnatomy files in anatomy dir
		brainFunction_files = glob.glob(os.path.join(timestep_directory, "brainFunction*.txt"))
##		brainFunction_files.sort(lambda x, y: cmp(int(os.path.basename(x)[14:-4]),
##							  int(os.path.basename(y)[14:-4])))
		#Extract critter number from brainFunction files
		number_list = map(lambda x: int(os.path.basename(x)[14:-4]), brainFunction_files)
                
                anatomy_directory = os.path.join(timestep_directory, "..", "..", "anatomy")
		brainAnatomy_files_all = glob.glob(os.path.join(anatomy_directory, "*death.txt"))

                #Pick out those brainAnatomy files with corresponding critter number to brainFunction files.
                brainAnatomy_files = []
		for anatomyFile in brainAnatomy_files_all:
                        if int(os.path.basename(anatomyFile)[13:-10]) in number_list:
                                brainAnatomy_files.append(anatomyFile)
                                               
		brainAnatomy_files.sort(lambda x, y: cmp(int(os.path.basename(x)[13:-10]),
							  int(os.path.basename(y)[13:-10])))
		if len(brainAnatomy_files) == 0:
			err('No brainanatomy files found in %s' % anatomy_directory)

##		cmd="%s brainfunction --bare --list %s -- %s" % ( CALC_COMPLEXITY,
##								  ' '.join(brainFunction_files),
##								  ' '.join(complexities_remaining))
##			
##		stdout = os.popen(cmd)
##		complexity_all = stdout.readlines()
##		if stdout.close() != None:
##			err('Failed executing CalcComplexity!')

                metric_all = calc_metrics(metrics_remaining, brainAnatomy_files)
	
	
		colnames = ['CritterNumber', 'Metric']
		coltypes = ['int', 'float']
		tables = dict([(type, datalib.Table(type, colnames, coltypes))
			       for type in metrics_remaining])
	
		# --- Parse the output
		for line in metric_all:
			fields = line.split()
			critter_number = fields.pop(0)

			for type in metrics_remaining:
				table = tables[type]
				row = table.createRow()
	
				row.set('CritterNumber', critter_number)
				row.set('Metric', fields.pop(0))
	
		# --- Write to file and normalize data (eg sort and remove 0's)
		for type in metrics_remaining:
			table = tables[type]
	
			datalib.write(__path(type),
				      table)
	
			data = table.getColumn('Metric').data
	
			tdata[type] = common_metric.normalize_metrics(data)

	return metrics


####################################################################################
###
### FUNCTION calc_metrics()
###
####################################################################################
def calc_metrics(metrics,anatomy_files):
        global RandomFlag
        metric_all = []
        for anatomy_file in anatomy_files:
                individual_metric = os.path.basename(anatomy_file)[13:-10] + ' '#Critter Number
                #metric_all.append(anatomy_file[13:-10])

                #Transform the anatomy file content into numpy adjacency matrix
                infile = open(anatomy_file,"r")
                lines = infile.readlines()
                lines.pop(0)

                H = []
                for l in lines:
                    g = []
                    l = l.split()
                    l.remove(';')
                    for e in l:
                        g.append(abs(float(e)))

                    H.append(g)


                B = numpy.matrix(H)
                G_Dir = nx.from_numpy_matrix(B,nx.DiGraph())#Geneterate directed version of brain network
                G_Undir = nx.Graph(G_Dir)#Generate undirected version of brain network
                if not RandomFlag:
                
                        if 'CC' in metrics:#need to calculate average clustering coeffcient
                                avr_clustering = nx.average_clustering(G_Undir)
                                individual_metric += str(avr_clustering) + ' '

                        if 'SP' in metrics:#need to calculate average shortest path length
                                avr_shortest_path_length = nx.normalized_average_shortest_path_length(G_Undir,weighted=False)
                                #avr_shortest_path_length = nx.normalized_average_shortest_path_length(G_Dir,weighted=True)
                                individual_metric += str(avr_shortest_path_length) + ' '
        ##                        closeness_centrality_list = nx.closeness_centrality(G_Undir).values()
        ##                        avr_closeness = sum(closeness_centrality_list) / len(closeness_centrality_list)
        ##                        individual_metric += str(avr_closeness) + ' '
                else:
                        num_nodes = G_Undir.number_of_nodes()
                        num_edges = G_Undir.number_of_edges()
                        Random_G_Undir = nx.gnm_random_graph(num_nodes,num_edges)
                        if 'CC' in metrics:#need to calculate average clustering coeffcient
                                avr_clustering = nx.average_clustering(Random_G_Undir)
                                individual_metric += str(avr_clustering) + ' '

                        if 'SP' in metrics:#need to calculate average shortest path length
                                avr_shortest_path_length = nx.normalized_average_shortest_path_length(Random_G_Undir,weighted=False)
                                #avr_shortest_path_length = nx.normalized_average_shortest_path_length(G_Dir,weighted=True)
                                individual_metric += str(avr_shortest_path_length) + ' '
        ##                        closeness_centrality_list = nx.closeness_centrality(G_Undir).values()
        ##                        avr_closeness = sum(closeness_centrality_list) / len(closeness_centrality_list)
        ##                        individual_metric += str(avr_closeness) + ' '
                        


                metric_all.append(individual_metric)

        return metric_all
                        
			 
####################################################################################
###
### FUNCTION make_percents()
###
####################################################################################
def make_percents(tables, totals):
	for table in tables.values():
		for row in table.rows():
			t = row.get('Timestep')
			total = totals[t][table.name]

			for bin in range(NUMBINS):
				row.mutate(bin, lambda x: 100 * x / total)

####################################################################################
###
### FUNCTION compute_bins()
###
####################################################################################
def compute_bins(DATA, timesteps, complexities, AVR, minfunc, maxfunc):
	totals = dict([(t, {}) for t in timesteps])
	
	colnames = ['Timestep'] + [x for x in range(NUMBINS)]
	coltypes = ['int'] + ['int' for x in range(NUMBINS)]
	tables = {}
	
	for type in complexities:
		table_bins = datalib.Table(type, colnames, coltypes)
		tables[type] = table_bins
	
		table_avr = AVR[type]
	
		for row_avr in table_avr.rows():
			t = row_avr.get('Timestep')
			minimum = minfunc(row_avr)
			maximum = maxfunc(row_avr)
	
			row_bins = table_bins.createRow()
			row_bins.set('Timestep', t)
			for bin in range(NUMBINS): row_bins.set(bin, 0)
	
			data = DATA[t][type]
	
			totals[t][type] = 0
	
			if(maximum > minimum):
				for complexity in data:
					bin = int(((complexity - minimum)/(maximum - minimum))*NUMBINS)
					if bin >= NUMBINS:
						bin -= 1
					row_bins.mutate(bin, lambda x: x + 1)
	
					totals[t][type] += 1
	
	make_percents(tables, totals)

	return tables
			
####################################################################################
###
### FUNCTION show_usage()
###
####################################################################################
def show_usage(msg = None):
################################################################################
	print"""\
USAGE

  %s [<options>]... (%s) <directory>...

DESCRIPTION

     Analyzes the complexities of the epochs contained in one or more recent
  directories.

     <directory> can specify either a run directory or a parent of one or more
  run directories.

    The results of the analysis are written to AvrComplexity.plt, 
  ComplexityHistNorm.plt, and ComplexityHistRaw.plt in the appropriate recent
  directory of each respective run directory. The generated files contain
  statistics on the complexity at each timestep.

OPTIONS

     -C <C>[,<C>]...
               Override default NeuralComplexity types for analysis, where C can
            be a composite of types (e.g. HB). Multiple C specs are separated by
            commas (e.g. -C P,HB,I).
            (default %s)

     -l, --legacy (force|prefer|off)
               force: Do not perform complexity calculations, but
            instead use results from run of older version of this script.

               prefer: Use results from older version if available,
            otherwise perform complexity calculations.

               off: Do not use any legacy files.

            (default off)

            Note: Legacy file paths are of the form
             <recent_directory>/<epoch>/complexity_<C>.txt.

     -b, --bins <n>
               Specify number of bins used in histogram calculations.
	    (default %d)""" % (sys.argv[0],
			       '|'.join(common_functions.RECENT_TYPES),
			       ','.join(common_metric.DEFAULT_METRICS),
			       common_metric.DEFAULT_NUMBINS)

        if msg:
            print "--------------------------------------------------------------------------------"
            print
            print 'Error!', msg
	sys.exit(1)

####################################################################################
###
### Primary Code Path
###
####################################################################################

exit_value = main()

sys.exit(exit_value)
