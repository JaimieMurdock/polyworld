#!/usr/bin/python

CurrentVersion=2

### Configurable parameters
OutputFilename2='MetricHist.plt'

#CALC_COMPLEXITY_OVER_DIRECTORY="./CalcComplexityOverDirectory.sh"
#### Don't modify anything beneath here unless you know what you're doing
import algorithms
import getopt, glob, string, sys, os
import common_functions
from common_functions import err, warn, list_difference
import common_metric
import datalib
import networkx as nx
import numpy

### Now initialize some global variables
##OutputFilename = common_complexity.FILENAME_AVR
OutputFilename = common_metric.FILENAME_AVR
OutputFilename1 = common_metric.RANDOMFILENAME_AVR
NUMBINS = common_metric.DEFAULT_NUMBINS
RandomN = 0
DefaultRecent = 'Recent'
LEGACY_MODES = ['force','prefer','off']
LegacyMode = 'off'
####

####################################################################################
###
### main()
###
####################################################################################
def main():
	#check_environment()

	metrics, recent_type, arg_paths = parse_args(sys.argv[1:])

	recent_subpath = os.path.join('brain', recent_type)
	try:
		run_paths = common_functions.find_run_paths(arg_paths,
							    recent_subpath)
	except common_functions.InvalidDirError, e:
		show_usage(str(e))

	recent_dirs = map(lambda x: os.path.join(x, recent_subpath),
			  run_paths)

	for recent_dir in recent_dirs:
		analyze_recent_dir(metrics,
				   recent_dir)

	print "Done!"
	return 0

####################################################################################
###
### FUNCTION check_environment()
###
####################################################################################
def check_environment():
	global CALC_COMPLEXITY

	CALC_COMPLEXITY = common_functions.pw_env('complexity')

####################################################################################
###
### FUNCTION parse_args()
###
####################################################################################
def parse_args(argv):
	global NUMBINS, LegacyMode, RandomN

	if len(argv) == 0:
		show_usage()

	metrics = common_metric.DEFAULT_METRICS#'CC' represents clustering coefficient
	                              #and 'SP' represents shortest path length


	recent_type = DefaultRecent

	short = 'D:V:R:'
	long = []

	try:
		opts, args = getopt.getopt(argv, short, long)
	except getopt.GetoptError, e:
		show_usage(str(e))

	for opt, value in opts:
		opt = opt.strip('-')

		if opt == 'D':
			try:
				recent_type = common_functions.expand_abbreviations( value,
										     common_functions.RECENT_TYPES,
										     case_sensitive = False )
			except common_functions.IllegalAbbreviationError, x:
				err(str(x))
		elif opt == 'V':
			metrics = map(string.upper, value.split(','))
		elif opt == 'R':
			try:
				RandomN = int(value)
				if RandomN < 0:
					show_usage("R argument must be >= 0")
			except:
				show_usage("R argument must be an integer >= 0")
##		elif opt == 'b' or opt == 'bins':
##			try:
##				NUMBINS = int(value)
##			except:
##				show_usage("Invalid integer argument for --bins (%s)" % value)
		else:
			assert(False)

	if len(args) == 0:
		show_usage('Must specify run/run-parent directory.')

	paths = list(args)

	return metrics, recent_type, paths

####################################################################################
###
### analyze_recent_dir()
###
####################################################################################
def analyze_recent_dir(metrics, recent_dir):
        global RandomN

	__analyze_recent_dir(metrics, recent_dir, random = False)
	if RandomN > 0:
		__analyze_recent_dir(metrics, recent_dir, random = True)



####################################################################################
###
### analyze_recent_dir()
###
####################################################################################
def __analyze_recent_dir(metrics, recent_dir, random):
        global RandomN

        if random:
                outputpath = os.path.join(recent_dir, "AvrMetricRandom.plt" )
        else:
                outputpath = os.path.join(recent_dir, OutputFilename)
                        
	
	print "- recent directory='%s'" %(recent_dir)
	print "- output='%s'" % (outputpath)

	#-----------------------------------------------------------------------------------
	#--
	#-- Find epoch/timestep directories
	#--
	#-----------------------------------------------------------------------------------
	timesteps = []
	# list all of the timesteps, make sure they are all integers (and directories), then sort them by number.
	for potential_timestep in os.listdir( recent_dir ):
		if not potential_timestep.isdigit(): continue					# if timestep IS NOT a digit (note, 0 is considered a digit), skip.
		if not os.path.isdir( os.path.join(recent_dir, potential_timestep) ): continue	# if the timestep isn't a directory, skip it.
	
		timesteps.append( int(potential_timestep) )						# add timestep to our list
	
	if len(timesteps) == 0:
		err('No epochs found. Not a valid recent directory.')

	timesteps.sort()									# sort the timesteps, lowest numbers come first.
	
	#-----------------------------------------------------------------------------------
	#--
	#-- Compute complexities for all timesteps
	#--
	#-- (store values to file in timestep dir)
	#--
	#-----------------------------------------------------------------------------------
	DATA={ }
	
	print "Final Timestep: %s" % ( max(timesteps) )
	print "Processing:",
	for t in timesteps:
		timestep_directory = os.path.join(recent_dir, str(t))
		print '%s...\n' % (t),
		sys.stdout.flush()	
	
		DATA[t] = tdata = {}

		metrics_remaining = metrics

		if len(metrics_remaining) > 0:
			metrics_computed = compute_metrics(metrics_remaining,
							   timestep_directory,
							   tdata,
							   random)
			metrics_remaining = list_difference(metrics_remaining,
							    metrics_computed)

		assert(len(metrics_remaining) == 0)

	#-----------------------------------------------------------------------------------
	#--
	#-- Create 'Avr' File
	#--
	#-----------------------------------------------------------------------------------
	AVR = algorithms.avr_table(DATA,
                                   metrics,
				   timesteps)
	
	datalib.write(outputpath, AVR)
	
	
##	#-----------------------------------------------------------------------------------
##	#--
##	#-- Create 'Norm' file
##	#--
##	#-----------------------------------------------------------------------------------
##	tables = compute_bins(DATA,
##			      timesteps,
##			      metrics,
##			      AVR,
##			      lambda row: row.get('min'),
##			      lambda row: row.get('max'))
##	
##	outputpath = os.path.join(recent_dir, OutputFilename2.replace( '.', 'Norm.'))
##	
##	datalib.write(outputpath, tables)
##	
##	
##	#-----------------------------------------------------------------------------------
##	#--
##	#-- Create 'Raw' file
##	#--
##	#-----------------------------------------------------------------------------------
##	MAXGLOBAL = dict([(type, float('-inf')) for type in metrics])
##	MINGLOBAL = dict([(type, float('inf')) for type in metrics])
##	
##	for avr_table in AVR.values():
##		for row in avr_table.rows():
##			type = avr_table.name
##	
##			MAXGLOBAL[type] = max(MAXGLOBAL[type], row.get('max'));
##			MINGLOBAL[type] = min(MINGLOBAL[type], row.get('min'));
##	
##	tables = compute_bins(DATA,
##			      timesteps,
##			      metrics,
##			      AVR,
##			      lambda row: MINGLOBAL[row.table.name],
##			      lambda row: MAXGLOBAL[row.table.name])
##	
##	outputpath = os.path.join(recent_dir, OutputFilename2.replace( '.', 'Raw.'))
##	
##	datalib.write(outputpath, tables)
##	

####################################################################################
###
### FUNCTION compute_metrics()
###
####################################################################################
def compute_metrics(metrics,
		    timestep_directory,
		    tdata,
		    random):
	global RandomN

	def __path(type):
		suffix = '_Random' if random else ''
		return os.path.join(timestep_directory, 'metric_' + type + suffix + '.plt')

	# --- Read in any metrics computed on a previous invocation of this script
	metrics_read = []
	if not random:
		for type in metrics:
			path = __path(type)

			if os.path.isfile(path):
				try:
					table = datalib.parse(path)[type]
					try:
						data = table.getColumn('Mean').data
					except KeyError, x:
						# Maybe it's of an older format
						data = table.getColumn('Metric').data
					tdata[type] = common_metric.normalize_metrics(data)
	
					metrics_read.append(type)
				except datalib.InvalidFileError, e:
					# file must have been incomplete
					print "Failed reading ", path, "(", e, ") ... regenerating"
	
	    
	metrics_remaining = list_difference(metrics,
					    metrics_read)
	
	print "  AlreadyHave =", metrics_read
	print "  MetricsToGet =", metrics_remaining

	# --- Compute metrics not already found on the file system
	if metrics_remaining:
		# --- Execute CalcMetric on all corresponding brainAnatomy files in anatomy dir
		brainFunction_files = glob.glob(os.path.join(timestep_directory, "brainFunction*.txt"))
##		brainFunction_files.sort(lambda x, y: cmp(int(os.path.basename(x)[14:-4]),
##							  int(os.path.basename(y)[14:-4])))
		#Extract critter number from brainFunction files
		number_list = map(lambda x: int(os.path.basename(x)[14:-4]), brainFunction_files)
                
                anatomy_directory = os.path.join(timestep_directory, "..", "..", "anatomy")
		brainAnatomy_files_all = glob.glob(os.path.join(anatomy_directory, "*death.txt"))

                #Pick out those brainAnatomy files with corresponding critter number to brainFunction files.
                brainAnatomy_files = []
		for anatomyFile in brainAnatomy_files_all:
                        if int(os.path.basename(anatomyFile)[13:-10]) in number_list:
                                brainAnatomy_files.append(anatomyFile)
                                               
		brainAnatomy_files.sort(lambda x, y: cmp(int(os.path.basename(x)[13:-10]),
							  int(os.path.basename(y)[13:-10])))
		if len(brainAnatomy_files) == 0:
			err('No brainanatomy files found in %s' % anatomy_directory)

                metric_all = calc_metrics(metrics_remaining, brainAnatomy_files, random)
	
		colnames = ['AgentNumber', 'SampSize', 'Mean', 'StdDev']
		coltypes = ['int', 'int', 'float', 'float']

		tables = dict([(type, datalib.Table(type, colnames, coltypes))
			       for type in metrics_remaining])

		for agent_results in metric_all:
			agent = agent_results['agent']
			results = agent_results['metrics']

			for type in metrics_remaining:
				result = results[type]
	
				n = len(result)
				mean, stddev, stderr = algorithms.sample_mean( result )
			
				table = tables[type]
				row = table.createRow()

				row['AgentNumber'] = agent
				row['SampSize'] = n
				row['Mean'] = mean
				row['StdDev'] = stddev
	
		# --- Write to file and normalize data (eg sort and remove 0's)
		for type in metrics_remaining:
			table = tables[type]
	
			datalib.write(__path(type),
				      table)
	
			data = table.getColumn('Mean').data
	
			tdata[type] = common_metric.normalize_metrics(data)

	return metrics


####################################################################################
###
### FUNCTION calc_metrics()
###
####################################################################################
def calc_metrics(metrics,anatomy_files, random):
        global RandomN
        metric_all = []

        for anatomy_file in anatomy_files:
		agent = os.path.basename(anatomy_file)[13:-10]
		result = dict([(metric, [])
			       for metric in metrics])

                #Transform the anatomy file content into numpy adjacency matrix
                infile = open(anatomy_file,"r")
                lines = infile.readlines()
                lines.pop(0)

                H = []
                for l in lines:
                    g = []
                    l = l.split()
                    l.remove(';')
                    for e in l:
                        g.append(abs(float(e)))

                    H.append(g)

                B = numpy.matrix(H)
                #G_Dir = nx.from_numpy_matrix(B,nx.DiGraph())#Geneterate directed version of brain network
                G_Undir = nx.from_numpy_matrix(B,nx.Graph())#Generate undirected version of brain network

		N = RandomN if random else 1

		for i in range(N):
			if random:
				num_nodes = G_Undir.number_of_nodes()
				num_edges = G_Undir.number_of_edges()
				G = nx.gnm_random_graph(num_nodes,num_edges)
			else:
				G = G_Undir

                        if 'CC' in metrics:
				#need to calculate average clustering coeffcient
                                avr_clustering = nx.average_clustering(G)
				result['CC'].append( avr_clustering )

                        if 'SP' in metrics:
				#need to calculate average shortest path length
                                avr_shortest_path_length = nx.normalized_average_shortest_path_length(G,weighted=False)
				result['SP'].append( avr_shortest_path_length )                        

		metric_all.append({'agent': agent,
				   'metrics': result})


        return metric_all
                        
			 
####################################################################################
###
### FUNCTION make_percents()
###
####################################################################################
def make_percents(tables, totals):
	for table in tables.values():
		for row in table.rows():
			t = row.get('Timestep')
			total = totals[t][table.name]

			for bin in range(NUMBINS):
				row.mutate(bin, lambda x: 100 * x / total)

####################################################################################
###
### FUNCTION compute_bins()
###
####################################################################################
def compute_bins(DATA, timesteps, complexities, AVR, minfunc, maxfunc):
	totals = dict([(t, {}) for t in timesteps])
	
	colnames = ['Timestep'] + [x for x in range(NUMBINS)]
	coltypes = ['int'] + ['int' for x in range(NUMBINS)]
	tables = {}
	
	for type in complexities:
		table_bins = datalib.Table(type, colnames, coltypes)
		tables[type] = table_bins
	
		table_avr = AVR[type]
	
		for row_avr in table_avr.rows():
			t = row_avr.get('Timestep')
			minimum = minfunc(row_avr)
			maximum = maxfunc(row_avr)
	
			row_bins = table_bins.createRow()
			row_bins.set('Timestep', t)
			for bin in range(NUMBINS): row_bins.set(bin, 0)
	
			data = DATA[t][type]
	
			totals[t][type] = 0
	
			if(maximum > minimum):
				for complexity in data:
					bin = int(((complexity - minimum)/(maximum - minimum))*NUMBINS)
					if bin >= NUMBINS:
						bin -= 1
					row_bins.mutate(bin, lambda x: x + 1)
	
					totals[t][type] += 1
	
	make_percents(tables, totals)

	return tables
			
####################################################################################
###
### FUNCTION show_usage()
###
####################################################################################
def show_usage(msg = None):
################################################################################
	print"""\
USAGE

  %s [<options>]... <directory>...

DESCRIPTION

     Analyzes the complexities of the epochs contained in one or more recent
  directories.

     <directory> can specify either a run directory or a parent of one or more
  run directories.

OPTIONS

     -D <dataset>
            Valid datasets:\
%s

            Note that datasets may be abbreviated.
            (default %s)

     -V <V>[,<V>]...
               Override default metrics, where V is one of:

               CC - Clustering Coefficient
               SP - (Normalized) Shortest Path length

            (default %s)""" % (sys.argv[0],
			       '\n               '.join(['']+common_functions.RECENT_TYPES),
			       DefaultRecent,
			       ','.join(common_metric.DEFAULT_METRICS))

        if msg:
            print "--------------------------------------------------------------------------------"
            print
            print 'Error!', msg
	sys.exit(1)

####################################################################################
###
### Primary Code Path
###
####################################################################################

exit_value = main()

sys.exit(exit_value)
